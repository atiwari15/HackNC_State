import cv2
import dlib
import numpy as np
import time
from collections import deque
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# ================================
# Setup: Face Detector, Landmark Predictor
# ================================
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

# Eye landmark indices from dlibâ€™s 68-point model.
LEFT_EYE = [36, 37, 38, 39, 40, 41]
RIGHT_EYE = [42, 43, 44, 45, 46, 47]

# Blink detection settings.
EAR_THRESHOLD = 0.25
CONSECUTIVE_FRAMES_FOR_BLINK = 2
blink_counter = 0
blink_detected = False

# Calibration settings
calibration_mode = True
calibration_points = [
    "top-left", "top-center", "top-right",
    "left-center", "center", "right-center",
    "bottom-left", "bottom-center", "bottom-right"
]
calibration_index = 0
calibration_data = []

# Polynomial regression for gaze mapping
poly = PolynomialFeatures(degree=2)
model_x = LinearRegression()
model_y = LinearRegression()

def get_eye_region(eye_points, landmarks):
    return [(landmarks.part(i).x, landmarks.part(i).y) for i in eye_points]

def get_gaze_ratio(eye_points, landmarks, frame, gray):
    eye_region = np.array(get_eye_region(eye_points, landmarks), dtype=np.int32)
    mask = np.zeros_like(gray)
    cv2.fillPoly(mask, [eye_region], 255)
    eye_frame = cv2.bitwise_and(frame, frame, mask=mask)
    min_x, max_x = np.min(eye_region[:, 0]), np.max(eye_region[:, 0])
    min_y, max_y = np.min(eye_region[:, 1]), np.max(eye_region[:, 1])
    eye_frame = eye_frame[min_y:max_y, min_x:max_x]
    if eye_frame.size == 0:
        return 0.5, 0.5
    eye_frame_blur = cv2.GaussianBlur(eye_frame, (7, 7), 0)
    hsv_eye = cv2.cvtColor(eye_frame_blur, cv2.COLOR_BGR2HSV)
    mask_dark = cv2.inRange(hsv_eye, (0, 0, 0), (180, 255, 50))
    contours, _ = cv2.findContours(mask_dark, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        contour = max(contours, key=cv2.contourArea)
        moments = cv2.moments(contour)
        cx = int(moments['m10'] / (moments['m00'] + 1e-6))
        cy = int(moments['m01'] / (moments['m00'] + 1e-6))
    else:
        cx, cy = eye_frame.shape[1] // 2, eye_frame.shape[0] // 2
    return cx / (eye_frame.shape[1] + 1e-6), cy / (eye_frame.shape[0] + 1e-6)

def calibrate_gaze_mapping():
    gaze_ratios = np.array([d[0] for d in calibration_data])
    screen_coords = np.array([d[1] for d in calibration_data])
    X_poly = poly.fit_transform(gaze_ratios)
    model_x.fit(X_poly, screen_coords[:, 0])
    model_y.fit(X_poly, screen_coords[:, 1])
    print("Calibration Complete.")

def map_gaze_to_screen(gaze_ratio_x, gaze_ratio_y):
    input_features = poly.transform([[gaze_ratio_x, gaze_ratio_y]])
    mapped_x = int(model_x.predict(input_features)[0])
    mapped_y = int(model_y.predict(input_features)[0])
    return mapped_x, mapped_y

cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    if not ret:
        break
    frame = cv2.flip(frame, 1)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = detector(gray)
    if faces:
        face = faces[0]
        landmarks = predictor(gray, face)
        left_ratio = get_gaze_ratio(LEFT_EYE, landmarks, frame, gray)
        right_ratio = get_gaze_ratio(RIGHT_EYE, landmarks, frame, gray)
        gaze_ratio_x = (left_ratio[0] + right_ratio[0]) / 2.0
        gaze_ratio_y = (left_ratio[1] + right_ratio[1]) / 2.0
    if calibration_mode:
        instruction_text = f"Look at {calibration_points[calibration_index]} and press 'c'"
        cv2.putText(frame, instruction_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
        cv2.imshow("Calibration", frame)
        key = cv2.waitKey(1) & 0xFF
        if key == ord('c'):
            screen_x = frame.shape[1] * (calibration_index % 3) / 2
            screen_y = frame.shape[0] * (calibration_index // 3) / 2
            calibration_data.append(((gaze_ratio_x, gaze_ratio_y), (screen_x, screen_y)))
            calibration_index += 1
            if calibration_index >= len(calibration_points):
                calibrate_gaze_mapping()
                calibration_mode = False
        continue
    mapped_x, mapped_y = map_gaze_to_screen(gaze_ratio_x, gaze_ratio_y)
    cv2.circle(frame, (mapped_x, mapped_y), 10, (0, 255, 0), -1)
    cv2.imshow("Eye Tracker", frame)
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()
